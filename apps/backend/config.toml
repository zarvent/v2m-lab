# voice2machine (v2m) - Configuration File

# LOAD PRIORITY (highest to lowest):
#   1. Constructor arguments (programmatic overrides)
#   2. Environment variables
#   3. .env file (local secrets)
#   4. THIS FILE (config.toml)
#   5. Defaults from Pydantic models
#
# SECURITY: Never commit secrets to this file.
# Use environment variables or .env for sensitive data.

# ============================================================================
# PATHS - XDG Base Directory Compliant
# ============================================================================
# All paths are auto-generated dynamically in code using XDG_RUNTIME_DIR
# Default location: /run/user/<uid>/v2m (or /tmp/v2m_<uid> as fallback)
# No configuration needed - defaults are correct.
[paths]

# ============================================================================
# TRANSCRIPTION SERVICE
# ============================================================================
[transcription]
backend = "whisper"  # Backend selector: "whisper" (future: "vosk", "custom")

[transcription.whisper]
model = "large-v3-turbo"
language = "auto"
device = "cuda"
# OPTIMIZATION for RTX 3060 (Ampere): "int8_float16"
# Weights in int8 (low VRAM), Compute in float16 (Tensor Cores).
# This is the "Asian/Torvalds" sweet spot: 2x less RAM, essentially lossless.
compute_type = "int8_float16"
device_index = 0
num_workers = 2  # 2 workers sufficient for single stream

# Inference parameters (state-of-the-art 2026)
# OPTIMIZATION: Beam size 1 (greedy) provides 3-5x speedup with minimal WER increase for clear audio.
# "Performance is Design" - Instant feedback is prioritized.
beam_size = 1
best_of = 1    # No candidates ranking (fastest)
temperature = [0.0, 0.2]  # Reduced fallback chain for faster failure/retry
vad_filter = true  # Enable faster-whisper native VAD

# VAD parameters (optimized for natural speech)
[transcription.whisper.vad_parameters]
threshold = 0.5              # Probability threshold 0.0-1.0 (higher = stricter detection)
min_speech_duration_ms = 250  # Minimum speech segment duration (filters brief noise)
min_silence_duration_ms = 2000  # Minimum silence for utterance end (2s allows natural pauses)

# ============================================================================
# LLM SERVICE
# ============================================================================
[gemini]
model = "gemini-3-flash-preview"  # Latest: Gemini 3 Flash (Dec 17, 2025) | Alt: "gemini-2.5-flash"
temperature = 0.3  # 0.0 (deterministic) to 2.0 (creative)
max_tokens = 8192  # Gemini 3 Flash has 128k context - increased from 2048
max_input_chars = 12000  # Proportional to max_tokens increase
request_timeout = 30
retry_attempts = 3
retry_min_wait = 2
retry_max_wait = 10

# SECURITY: Load from environment variable (never commit API keys)
api_key = "${GEMINI_API_KEY}"

# ============================================================================
# DESKTOP NOTIFICATIONS
# ============================================================================
[notifications]
expire_time_ms = 3000  # Auto-close time in milliseconds (3s default)
auto_dismiss = true    # Programmatic close via DBUS (for Unity/GNOME that ignore expire-time)

# ============================================================================
# LOCAL LLM (Alternative to Gemini)
# ============================================================================
[llm]
backend = "local"  # Options: "local" (llama.cpp), "gemini" (Google API)

[llm.local]
model_path = "models/qwen2.5-3b-instruct-q4_k_m.gguf"
n_gpu_layers = -1   # -1 = full GPU offload (all layers to GPU for fastest inference)
n_ctx = 2048        # Context window in tokens
temperature = 0.3   # 0.0 (deterministic) to 2.0 (creative)
max_tokens = 512    # Maximum tokens to generate

# ============================================================================
# OLLAMA LLM (Alternative local backend with structured outputs)
# ============================================================================
[llm.ollama]
host = "http://localhost:11434"
model = "gemma2:2b"      # ALT: "phi3.5-mini", "qwen2.5-coder:7b"
keep_alive = "5m"        # "0m" = free VRAM | "5m" = keep loaded | "30m" = min latency
temperature = 0.0        # 0.0 for deterministic structured outputs
